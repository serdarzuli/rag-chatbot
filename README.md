# 🤖 RAG Chatbot (LangChain + HuggingFace + Gradio, CPU)

This project builds a **Retrieval-Augmented Generation (RAG)** chatbot from your own text data using:

* LangChain for chaining components
* Hugging Face models for both embedding and generation
* ChromaDB for vector storage
* Gradio for a web-based chatbot interface

Everything runs **offline** and is optimized for **CPU-based inference**.

---

## 🚀 Features

* ✅ CPU-friendly HuggingFace generation model (e.g., `falcon-rw-1b`)
* ✅ Sentence-Transformer embedding via LangChain
* ✅ Chat memory with latest 2 exchanges
* ✅ Cleaned output: only final answer shown (e.g. after "Helpful Answer:")
* ✅ Simple Gradio web UI

---

## 📦 Requirements

```bash
pip install -r requirements.txt
```

---

## 📁 Project Structure

<details>
<summary><code>📁 rag-chatbot/</code></summary>

```
rag-chatbot/
├── src/
│   ├── __init__.py
|   ├── api.py
│   ├── config.py                # Loads .env
│   ├── rag_pipeline.py         # Embedding, retrieval, generation
│
├──frontend/
|   ├── ui.py                   # Gradio web UI
├── main.py                     # Optional FastAPI entrypoint
├── requirements.txt
├── .env
├── README.md
│
├── data/
│   └── dialogs.txt             # Your text data file
│
├── vector_db/                  # Chroma vector store (autogenerated)
```

</details>

---

## 🧾 Step-by-Step Usage

### 1. Add Your Data

Prepare a file at `data/dialogs.txt`. Example lines:

```
hey, how are you?
I'm good thanks.
do you like your school?
Yes, I do.
```

### 2. Build the Vector Store

```python
from app.rag_pipeline import ingest_and_index
ingest_and_index("data/dialogs.txt")
```

### 3. Launch the Chatbot UI

```bash
python gradio_ui.py
```

Then open:

```
http://localhost:7860
```

---

## 💬 Example Output

```
User: hey, how are you?
Bot: I'm good thanks.

User: do you like your school?
Bot: Yes, I do.
```

---

## 🧠 Notes

* Only the final helpful answer is returned (e.g., cleans "Helpful Answer:")
* Only last 2 rounds of chat are preserved for memory efficiency
* The LLM model must be causal (e.g. `text-generation`, not `text2text-generation`)

---


## 👨‍💻 Built With

> 💡 LangChain, HuggingFace, ChromaDB, Gradio, Python

---
