# ğŸ¤– RAG Chatbot (LangChain + HuggingFace + Gradio, CPU)

This project builds a **Retrieval-Augmented Generation (RAG)** chatbot from your own text data using:

* LangChain for chaining components
* Hugging Face models for both embedding and generation
* ChromaDB for vector storage
* Gradio for a web-based chatbot interface

Everything runs **offline** and is optimized for **CPU-based inference**.

---

## ğŸš€ Features

* âœ… CPU-friendly HuggingFace generation model (e.g., `falcon-rw-1b`)
* âœ… Sentence-Transformer embedding via LangChain
* âœ… Chat memory with latest 2 exchanges
* âœ… Cleaned output: only final answer shown (e.g. after "Helpful Answer:")
* âœ… Simple Gradio web UI

---

## ğŸ“¦ Requirements

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Project Structure

<details>
<summary><code>ğŸ“ rag-chatbot/</code></summary>

```
rag-chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
|   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ config.py                # Loads .env
â”‚   â”œâ”€â”€ rag_pipeline.py         # Embedding, retrieval, generation
â”‚
â”œâ”€â”€frontend/
|   â”œâ”€â”€ ui.py                   # Gradio web UI
â”œâ”€â”€ main.py                     # Optional FastAPI entrypoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dialogs.txt             # Your text data file
â”‚
â”œâ”€â”€ vector_db/                  # Chroma vector store (autogenerated)
```

</details>

---

## ğŸ§¾ Step-by-Step Usage

### 1. Add Your Data

Prepare a file at `data/dialogs.txt`. Example lines:

```
hey, how are you?
I'm good thanks.
do you like your school?
Yes, I do.
```

### 2. Build the Vector Store

```python
from app.rag_pipeline import ingest_and_index
ingest_and_index("data/dialogs.txt")
```

### 3. Launch the Chatbot UI

```bash
python gradio_ui.py
```

Then open:

```
http://localhost:7860
```

---

## ğŸ’¬ Example Output

```
User: hey, how are you?
Bot: I'm good thanks.

User: do you like your school?
Bot: Yes, I do.
```

---

## ğŸ§  Notes

* Only the final helpful answer is returned (e.g., cleans "Helpful Answer:")
* Only last 2 rounds of chat are preserved for memory efficiency
* The LLM model must be causal (e.g. `text-generation`, not `text2text-generation`)

---


## ğŸ‘¨â€ğŸ’» Built With

> ğŸ’¡ LangChain, HuggingFace, ChromaDB, Gradio, Python

---
